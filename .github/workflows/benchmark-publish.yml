# =============================================================================
# benchmark-publish.yml
#
# è‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•å‘å¸ƒæµæ°´çº¿
#
# è§¦å‘æ–¹å¼ï¼š
#   1. æ‰‹åŠ¨è§¦å‘ï¼ˆworkflow_dispatchï¼‰- æ”¯æŒè‡ªå®šä¹‰å‚æ•°
#   2. å®šæ—¶è§¦å‘ï¼ˆscheduleï¼‰          - é»˜è®¤æ¯å‘¨ä¸€ 02:00 UTC è‡ªåŠ¨è¿è¡Œ
#   3. æŽ¨é€å¸¦ [publish] æ ‡è®°çš„ commit
#
# å·¥ä½œæµç¨‹ï¼š
#   Step 1: è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼ˆself-hostedï¼Œéœ€æœåŠ¡å¯ç”¨ï¼‰
#   Step 2: èšåˆç»“æžœ â†’ hf_data/
#   Step 3: Schema éªŒè¯
#   Step 4: ä¸Šä¼ åˆ° HuggingFace Dataset
#   Step 5: è§¦å‘ sagellm-website æ›´æ–°ï¼ˆrepository_dispatchï¼‰
# =============================================================================

name: Benchmark Publish Pipeline

on:
  # æ‰‹åŠ¨è§¦å‘ï¼Œæ”¯æŒå‚æ•°é€‰æ‹©
  workflow_dispatch:
    inputs:
      model:
        description: 'Model to benchmark (HF repo or local path)'
        required: false
        default: 'sshleifer/tiny-gpt2'
        type: string
      workload:
        description: 'Workload(s) to run: Q1/Q2/.../Q8/all/year1'
        required: false
        default: 'all'
        type: string
      backend:
        description: 'Backend type'
        required: false
        default: 'cpu'
        type: choice
        options: [cpu, cuda, vllm]
      skip_run:
        description: 'Skip benchmark run (only aggregate+upload existing outputs)'
        required: false
        default: false
        type: boolean
      dry_run:
        description: 'Dry run (do not upload to HF)'
        required: false
        default: false
        type: boolean

  # å®šæ—¶è‡ªåŠ¨å‘å¸ƒï¼šæ¯å‘¨ä¸€ UTC 02:00ï¼ˆåŒ—äº¬æ—¶é—´ 10:00ï¼‰
  schedule:
    - cron: '0 2 * * 1'

  # commit æ¶ˆæ¯å« [publish] æ—¶è§¦å‘
  push:
    branches: [main, main-dev]
    paths:
      - 'outputs/**'
    # æˆ–è€…é€šè¿‡ commit message è§¦å‘
    # åœ¨ job ä¸­ç”¨ contains(github.event.head_commit.message, '[publish]') åˆ¤æ–­

env:
  HF_REPO: intellistream/sagellm-benchmark-results
  HF_ENDPOINT: ${{ vars.HF_ENDPOINT || 'https://huggingface.co' }}
  # é»˜è®¤å‚æ•°ï¼ˆschedule è§¦å‘æ—¶ä½¿ç”¨ï¼‰
  DEFAULT_MODEL: 'sshleifer/tiny-gpt2'
  DEFAULT_WORKLOAD: 'all'
  DEFAULT_BACKEND: 'cpu'

jobs:
  # -------------------------------------------------------------------------
  # Job 1: è¿è¡ŒåŸºå‡†æµ‹è¯• + èšåˆç»“æžœ
  # -------------------------------------------------------------------------
  run-and-aggregate:
    name: Run Benchmark & Aggregate
    runs-on: ubuntu-latest
    timeout-minutes: 60

    outputs:
      # æ˜¯å¦æœ‰æ–°ç»“æžœå¯ä¸Šä¼ 
      has_results: ${{ steps.check_results.outputs.has_results }}
      result_count: ${{ steps.check_results.outputs.result_count }}
      run_summary: ${{ steps.save_summary.outputs.summary }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Parse parameters (schedule fallback to defaults)
        id: params
        run: |
          # workflow_dispatch æœ‰ inputsï¼Œschedule/push æ²¡æœ‰ï¼Œç”¨é»˜è®¤å€¼
          MODEL="${{ github.event.inputs.model || env.DEFAULT_MODEL }}"
          WORKLOAD="${{ github.event.inputs.workload || env.DEFAULT_WORKLOAD }}"
          BACKEND="${{ github.event.inputs.backend || env.DEFAULT_BACKEND }}"
          SKIP_RUN="${{ github.event.inputs.skip_run || 'false' }}"
          DRY_RUN="${{ github.event.inputs.dry_run || 'false' }}"

          echo "model=${MODEL}"         >> "$GITHUB_OUTPUT"
          echo "workload=${WORKLOAD}"   >> "$GITHUB_OUTPUT"
          echo "backend=${BACKEND}"     >> "$GITHUB_OUTPUT"
          echo "skip_run=${SKIP_RUN}"   >> "$GITHUB_OUTPUT"
          echo "dry_run=${DRY_RUN}"     >> "$GITHUB_OUTPUT"

          echo "ðŸ“‹ Parameters:"
          echo "  model=${MODEL}  workload=${WORKLOAD}  backend=${BACKEND}"

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Configure mirrors (China network)
        if: contains(toJson(runner.labels), 'self-hosted')
        run: |
          mkdir -p ~/.pip
          cat > ~/.pip/pip.conf << 'EOF'
          [global]
          index-url = https://pypi.tuna.tsinghua.edu.cn/simple
          trusted-host = pypi.tuna.tsinghua.edu.cn
          EOF
          echo "HF_ENDPOINT=https://hf-mirror.com" >> $GITHUB_ENV

      - name: Install benchmark package (local source)
        run: |
          pip install -e . huggingface_hub

      - name: Run benchmark
        if: steps.params.outputs.skip_run != 'true'
        run: |
          echo "ðŸƒ Running benchmark..."
          sagellm-benchmark run \
            --workload "${{ steps.params.outputs.workload }}" \
            --backend "${{ steps.params.outputs.backend }}" \
            --model "${{ steps.params.outputs.model }}" \
            --verbose

      - name: Check results
        id: check_results
        run: |
          COUNT=$(find outputs/ -name "*_leaderboard.json" 2>/dev/null | wc -l || echo 0)
          echo "result_count=${COUNT}" >> "$GITHUB_OUTPUT"
          if [[ "${COUNT}" -gt 0 ]]; then
            echo "has_results=true" >> "$GITHUB_OUTPUT"
            echo "âœ“ Found ${COUNT} leaderboard JSON files"
            find outputs/ -name "*_leaderboard.json" | head -20
          else
            echo "has_results=false" >> "$GITHUB_OUTPUT"
            echo "âš ï¸ No leaderboard files found"
          fi

      - name: Aggregate results â†’ hf_data/
        if: steps.check_results.outputs.has_results == 'true'
        run: |
          echo "ðŸ”€ Aggregating results..."
          python scripts/aggregate_for_hf.py

          echo "ðŸ“Š Aggregated files:"
          for f in hf_data/leaderboard_single.json hf_data/leaderboard_multi.json; do
            if [[ -f "${f}" ]]; then
              COUNT=$(python3 -c "import json;d=json.load(open('${f}'));print(len(d))" 2>/dev/null || echo "?")
              echo "  âœ“ ${f} (${COUNT} records)"
            fi
          done

      - name: Validate schema
        if: steps.check_results.outputs.has_results == 'true'
        run: |
          if [[ -f "data/validate_schema.py" ]]; then
            python3 data/validate_schema.py hf_data/leaderboard_single.json && echo "âœ“ Schema valid" || echo "âš ï¸ Schema warnings (non-fatal)"
          else
            echo "â„¹ï¸ validate_schema.py not found, skipping"
          fi

      - name: Save run summary
        id: save_summary
        run: |
          SUMMARY="model=${{ steps.params.outputs.model }} workload=${{ steps.params.outputs.workload }} backend=${{ steps.params.outputs.backend }} count=${{ steps.check_results.outputs.result_count }}"
          echo "summary=${SUMMARY}" >> "$GITHUB_OUTPUT"

      - name: Upload hf_data as artifact
        if: steps.check_results.outputs.has_results == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: leaderboard-data
          path: hf_data/*.json
          retention-days: 30

  # -------------------------------------------------------------------------
  # Job 2: ä¸Šä¼ åˆ° HuggingFace
  # -------------------------------------------------------------------------
  upload-to-hf:
    name: Upload to HuggingFace
    runs-on: ubuntu-latest
    needs: run-and-aggregate
    if: |
      needs.run-and-aggregate.outputs.has_results == 'true' &&
      github.event.inputs.dry_run != 'true'
    timeout-minutes: 15

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: leaderboard-data
          path: hf_data/

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install huggingface_hub

      - name: Merge with latest HF data (concurrent-safe)
        env:
          HF_REPO: ${{ env.HF_REPO }}
        run: python scripts/merge_and_upload.py

      - name: Upload to HuggingFace
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_REPO: ${{ env.HF_REPO }}
        run: python scripts/upload_to_hf.py

      - name: Show result URL
        run: |
          echo ""
          echo "âœ… Upload complete!"
          echo "ðŸ”— Dataset: https://huggingface.co/datasets/${{ env.HF_REPO }}"
          echo "ðŸŒ Website: https://intellistream.github.io/sagellm-website"

  # -------------------------------------------------------------------------
  # Job 3: é€šçŸ¥ sagellm-website åˆ·æ–°ï¼ˆå¯é€‰ï¼‰
  # -------------------------------------------------------------------------
  notify-website:
    name: Notify Website
    runs-on: ubuntu-latest
    needs: upload-to-hf
    if: success()
    timeout-minutes: 5

    steps:
      - name: Dispatch website update event
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.CROSS_REPO_TOKEN || secrets.GITHUB_TOKEN }}
          script: |
            try {
              await github.rest.repos.createDispatchEvent({
                owner: 'intellistream',
                repo: 'sagellm-website',
                event_type: 'benchmark-data-updated',
                client_payload: {
                  summary: '${{ needs.run-and-aggregate.outputs.run_summary }}',
                  timestamp: new Date().toISOString(),
                  hf_repo: process.env.HF_REPO
                }
              });
              console.log('âœ“ Website notified');
            } catch (e) {
              console.log('âš ï¸ Could not notify website (non-fatal):', e.message);
            }

  # -------------------------------------------------------------------------
  # Job 4: å‘å¸ƒæ€»ç»“
  # -------------------------------------------------------------------------
  summary:
    name: Pipeline Summary
    runs-on: ubuntu-latest
    needs: [run-and-aggregate, upload-to-hf, notify-website]
    if: always()

    steps:
      - name: Print summary
        run: |
          echo "## ðŸ“Š Benchmark Publish Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| æ­¥éª¤ | çŠ¶æ€ |" >> $GITHUB_STEP_SUMMARY
          echo "|------|------|" >> $GITHUB_STEP_SUMMARY
          echo "| Run & Aggregate | ${{ needs.run-and-aggregate.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Upload to HF    | ${{ needs.upload-to-hf.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Notify Website  | ${{ needs.notify-website.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Run info:** ${{ needs.run-and-aggregate.outputs.run_summary }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ”— [HF Dataset](https://huggingface.co/datasets/${{ env.HF_REPO }}) | [Website](https://intellistream.github.io/sagellm-website)" >> $GITHUB_STEP_SUMMARY
