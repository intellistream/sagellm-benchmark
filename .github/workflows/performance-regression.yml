name: Performance Regression Check

on:
  pull_request:
    branches: [main, main-dev, dev]
    paths:
      - 'src/sagellm_benchmark/performance/**'
      - 'src/sagellm_benchmark/cli.py'
      - 'scripts/compare_performance_baseline.py'
      - 'benchmarks/baselines/**'
      - '.github/workflows/performance-regression.yml'
  push:
    branches: [main, main-dev]
    paths:
      - 'src/sagellm_benchmark/performance/**'
      - 'src/sagellm_benchmark/cli.py'
      - 'scripts/compare_performance_baseline.py'
      - 'benchmarks/baselines/**'
      - '.github/workflows/performance-regression.yml'
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

jobs:
  performance-regression:
    name: Perf Regression
    runs-on: self-hosted
    timeout-minutes: 20

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run current perf benchmark
        run: |
          mkdir -p benchmark_results
          sagellm-benchmark perf \
            --type e2e \
            --model Qwen/Qwen2-7B-Instruct \
            --batch-size 1 --batch-size 4 --batch-size 8 \
            --precision fp16 --precision int8 \
            --output-json benchmark_results/perf_current.json \
            --output-markdown benchmark_results/perf_current.md

      - name: Compare against baseline
        id: compare
        continue-on-error: true
        run: |
          python scripts/compare_performance_baseline.py \
            --baseline benchmarks/baselines/perf_baseline_e2e.json \
            --current benchmark_results/perf_current.json \
            --warning-threshold 5 \
            --critical-threshold 10 \
            --summary-json benchmark_results/perf_comparison_summary.json \
            --report-md benchmark_results/perf_comparison_report.md \
            --github-output "$GITHUB_OUTPUT"

      - name: Upload performance artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-regression-results
          path: |
            benchmark_results/perf_current.json
            benchmark_results/perf_current.md
            benchmark_results/perf_comparison_summary.json
            benchmark_results/perf_comparison_report.md

      - name: PR comment on warning/critical
        if: github.event_name == 'pull_request' && (steps.compare.outputs.overall_status == 'warning' || steps.compare.outputs.overall_status == 'critical')
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const status = `${{ steps.compare.outputs.overall_status }}`;
            const report = fs.readFileSync('benchmark_results/perf_comparison_report.md', 'utf8');
            const body = [
              '## ⚠️ Performance Regression Check',
              '',
              `Status: **${status.toUpperCase()}**`,
              '',
              report,
            ].join('\n');

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body,
            });

      - name: Fail on critical regression
        if: steps.compare.outputs.overall_status == 'critical'
        run: |
          echo "Critical performance regression detected (>10%)."
          exit 1
