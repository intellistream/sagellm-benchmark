"""ç«¯åˆ°ç«¯ç¤ºä¾‹ï¼šæ¼”ç¤º Aggregator + Contract + Reporters å®Œæ•´æµç¨‹ã€‚

ç”¨æ³•:
    python examples/task_c_demo.py
"""

from __future__ import annotations

from pathlib import Path

from sagellm_protocol import Metrics, Timestamps

from sagellm_benchmark.metrics import ContractVerifier, MetricsAggregator
from sagellm_benchmark.reporters import JSONReporter, MarkdownReporter, TableReporter
from sagellm_benchmark.types import BenchmarkResult, ContractVersion


def create_sample_results() -> list[BenchmarkResult]:
    """åˆ›å»º 5 ä¸ªç¤ºä¾‹ BenchmarkResultï¼ˆYear1 æ°´å¹³ï¼‰ã€‚"""
    results = []

    for i in range(5):
        timestamps = Timestamps(
            queued_at=1000.0 + i * 2.0,
            scheduled_at=1000.0 + i * 2.0 + 0.1,
            executed_at=1000.0 + i * 2.0 + 0.2,
            completed_at=1000.0 + i * 2.0 + 0.3,
        )

        metrics = Metrics(
            ttft_ms=10.0 + i * 5.0,  # 10, 15, 20, 25, 30 (avg=20ms)
            tbt_ms=2.0 + i * 1.0,  # 2, 3, 4, 5, 6 (avg=4ms)
            tpot_ms=2.5 + i * 0.5,  # 2.5, 3.0, 3.5, 4.0, 4.5 (avg=3.5ms)
            throughput_tps=100.0 - i * 10.0,  # 100, 90, 80, 70, 60 (avg=80 tokens/s)
            peak_mem_mb=1024 + i * 256,  # 1024 ~ 2048 (max=2048)
            error_rate=0.0,  # æ— é”™è¯¯
            kv_used_tokens=128 + i * 32,  # 128 ~ 256
            kv_used_bytes=(128 + i * 32) * 16,
            prefix_hit_rate=0.8 + i * 0.02,  # 0.8 ~ 0.88 (avg=0.84)
            evict_count=i,  # 0, 1, 2, 3, 4 (sum=10)
            evict_ms=0.5 * i,  # 0, 0.5, 1.0, 1.5, 2.0 (sum=5.0ms)
            spec_accept_rate=0.7 + i * 0.01,  # 0.7 ~ 0.74 (avg=0.72)
            timestamps=timestamps,
        )

        result = BenchmarkResult(
            request_id=f"req-{i:03d}",
            success=True,
            error=None,
            metrics=metrics,
            output_text=f"Generated output for request {i}",
            output_tokens=50 + i * 10,  # 50, 60, 70, 80, 90 (total=350)
            prompt_tokens=100,
        )

        results.append(result)

    return results


def main() -> None:
    """è¿è¡Œå®Œæ•´çš„ Task C ç¤ºä¾‹ã€‚"""
    print("=" * 80)
    print("Task C ç«¯åˆ°ç«¯ç¤ºä¾‹ï¼šMetrics Aggregation & Reporting")
    print("=" * 80)
    print()

    # === æ­¥éª¤ 1: åˆ›å»ºç¤ºä¾‹æ•°æ® ===
    print("ğŸ“¦ Step 1: åˆ›å»º 5 ä¸ªç¤ºä¾‹ BenchmarkResult...")
    results = create_sample_results()
    print(f"âœ… æˆåŠŸåˆ›å»º {len(results)} ä¸ªè¯·æ±‚ç»“æœ")
    print()

    # === æ­¥éª¤ 2: èšåˆæŒ‡æ ‡ ===
    print("ğŸ“Š Step 2: èšåˆæŒ‡æ ‡...")
    aggregated = MetricsAggregator.aggregate(results)
    print("âœ… èšåˆå®Œæˆ")
    print(f"   - Avg TTFT: {aggregated.avg_ttft_ms:.2f}ms")
    print(f"   - P95 TTFT: {aggregated.p95_ttft_ms:.2f}ms")
    print(f"   - Avg Throughput: {aggregated.avg_throughput_tps:.2f} tokens/s")
    print(f"   - Error Rate: {aggregated.error_rate * 100:.2f}%")
    print()

    # === æ­¥éª¤ 3: Year1 Contract éªŒè¯ ===
    print("âœ… Step 3: Year1 Contract éªŒè¯...")
    year1_result = ContractVerifier.verify(aggregated, ContractVersion.YEAR1)
    print(f"   {year1_result.summary}")

    for check_name, passed in year1_result.checks.items():
        status = "âœ…" if passed else "âŒ"
        detail = year1_result.details.get(check_name, "")
        print(f"   {status} {check_name}: {detail}")
    print()

    # === æ­¥éª¤ 4: Year2 Contract éªŒè¯ï¼ˆé¢„æœŸéƒ¨åˆ†ä¸é€šè¿‡ï¼‰===
    print("ğŸ” Step 4: Year2 Contract éªŒè¯ï¼ˆæ›´ä¸¥æ ¼ï¼‰...")
    year2_result = ContractVerifier.verify(aggregated, ContractVersion.YEAR2)
    print(f"   {year2_result.summary}")

    for check_name, passed in year2_result.checks.items():
        status = "âœ…" if passed else "âŒ"
        detail = year2_result.details.get(check_name, "")
        print(f"   {status} {check_name}: {detail}")
    print()

    # === æ­¥éª¤ 5: ç”Ÿæˆ JSON æŠ¥å‘Š ===
    print("ğŸ’¾ Step 5: ç”Ÿæˆ JSON æŠ¥å‘Š...")
    output_dir = Path("./benchmark_results")
    output_dir.mkdir(exist_ok=True)

    json_path = output_dir / "task_c_demo.json"
    JSONReporter.generate(
        metrics=aggregated,
        contract=year1_result,
        output_path=json_path,
        version="0.1.0.2",
        timestamp="2026-01-17T10:30:00",
    )
    print(f"âœ… JSON æŠ¥å‘Šå·²ä¿å­˜: {json_path}")
    print()

    # === æ­¥éª¤ 6: ç”Ÿæˆ Markdown æŠ¥å‘Š ===
    print("ğŸ“ Step 6: ç”Ÿæˆ Markdown æŠ¥å‘Š...")
    md_path = output_dir / "task_c_demo.md"
    MarkdownReporter.generate(
        metrics=aggregated,
        contract=year1_result,
        output_path=md_path,
        title="Task C Demo - Benchmark Report",
        version="0.1.0.2",
    )
    print(f"âœ… Markdown æŠ¥å‘Šå·²ä¿å­˜: {md_path}")
    print()

    # === æ­¥éª¤ 7: ç»ˆç«¯è¡¨æ ¼è¾“å‡º ===
    print("ğŸ“‹ Step 7: ç»ˆç«¯è¡¨æ ¼è¾“å‡ºï¼ˆRichï¼‰")
    print("-" * 80)
    TableReporter.generate(
        metrics=aggregated,
        contract=year1_result,
        show_contract=True,
    )

    print("=" * 80)
    print("âœ… Task C ç¤ºä¾‹å®Œæˆï¼")
    print(f"ğŸ“ æŠ¥å‘Šè¾“å‡ºç›®å½•: {output_dir.absolute()}")
    print("=" * 80)


if __name__ == "__main__":
    main()
