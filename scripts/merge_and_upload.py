#!/usr/bin/env python3
"""
å¹¶å‘å®‰å…¨çš„åˆå¹¶å’Œä¸Šä¼ è„šæœ¬

ç”¨äº GitHub Actionsï¼Œåœ¨ä¸Šä¼ åˆ° HF å‰å†æ¬¡åˆå¹¶æœ€æ–°æ•°æ®ï¼Œè§£å†³å¹¶å‘å†²çªã€‚

å·¥ä½œæµç¨‹ï¼š
1. è¯»å–ç”¨æˆ·æäº¤çš„ hf_data/ï¼ˆå¯èƒ½åŸºäºæ—§ç‰ˆæœ¬ HF æ•°æ®ï¼‰
2. ä» HF ä¸‹è½½æœ€æ–°æ•°æ®ï¼ˆå¯èƒ½å·²è¢«å…¶ä»–ç”¨æˆ·æ›´æ–°ï¼‰
3. ä¸‰æ–¹æ™ºèƒ½åˆå¹¶ï¼ˆä»¥ HF æœ€æ–°ç‰ˆæœ¬ä¸ºåŸºå‡†ï¼‰
4. ä¿å­˜åˆå¹¶ç»“æœï¼ˆä¾› upload_to_hf.py ä½¿ç”¨ï¼‰

è¿™æ ·å³ä½¿å¤šä¸ªç”¨æˆ·å¹¶å‘æäº¤ï¼Œä¹Ÿä¸ä¼šä¸¢å¤±æ•°æ®ã€‚
"""

from __future__ import annotations

import json
import os
import re
import urllib.request
from pathlib import Path

# HF é…ç½®
HF_REPO = "intellistream/sagellm-benchmark-results"
HF_BRANCH = "main"


def download_from_hf(filename: str) -> list[dict]:
    """
    ä» HF ä¸‹è½½æœ€æ–°æ•°æ®ï¼ˆå…¬å¼€ï¼Œæ— éœ€ tokenï¼‰

    ç«¯ç‚¹é€‰æ‹©ç­–ç•¥ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
    1. ç¯å¢ƒå˜é‡ HF_ENDPOINTï¼ˆå¦‚æœè®¾ç½®ï¼‰
    2. å®˜æ–¹åœ°å€ https://huggingface.coï¼ˆé»˜è®¤ï¼‰
    3. å¦‚æœå®˜æ–¹å¤±è´¥ï¼Œè‡ªåŠ¨å›é€€åˆ° https://hf-mirror.com
    """
    # 1. ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„ç«¯ç‚¹
    endpoint = os.getenv("HF_ENDPOINT", "https://huggingface.co")

    # 2. å®šä¹‰å¤‡ç”¨ç«¯ç‚¹åˆ—è¡¨ï¼ˆå¦‚æœä¸»ç«¯ç‚¹å¤±è´¥ï¼‰
    fallback_endpoints = []
    if endpoint != "https://hf-mirror.com":
        # å¦‚æœå½“å‰ä¸æ˜¯é•œåƒï¼Œå°†é•œåƒä½œä¸ºå¤‡ç”¨
        fallback_endpoints.append("https://hf-mirror.com")
    if endpoint != "https://huggingface.co":
        # å¦‚æœå½“å‰ä¸æ˜¯å®˜æ–¹ï¼Œå°†å®˜æ–¹ä½œä¸ºå¤‡ç”¨
        fallback_endpoints.append("https://huggingface.co")

    # 3. å°è¯•ä¸»ç«¯ç‚¹
    endpoints_to_try = [endpoint] + fallback_endpoints

    for idx, ep in enumerate(endpoints_to_try):
        url = f"{ep}/datasets/{HF_REPO}/resolve/{HF_BRANCH}/{filename}"
        is_primary = idx == 0
        prefix = "  ğŸ“¥" if is_primary else "  ğŸ”„ å›é€€åˆ°"

        print(f"{prefix} {url}")

        try:
            with urllib.request.urlopen(url, timeout=30) as response:
                data = json.loads(response.read().decode("utf-8"))
                print(f"    âœ“ {len(data)} æ¡è®°å½•")
                return data
        except urllib.error.HTTPError as e:
            if e.code == 404:
                print("    âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆé¦–æ¬¡ä¸Šä¼ ï¼‰")
                return []  # 404 æ˜¯ç¡®å®šçš„ï¼Œæ— éœ€é‡è¯•
            else:
                print(f"    âš ï¸ HTTP {e.code}: {e.reason}")
                if idx < len(endpoints_to_try) - 1:
                    continue  # å°è¯•ä¸‹ä¸€ä¸ªç«¯ç‚¹
                return []
        except Exception as e:
            print(f"    âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
            if idx < len(endpoints_to_try) - 1:
                continue  # å°è¯•ä¸‹ä¸€ä¸ªç«¯ç‚¹
            return []

    return []


def get_config_key(entry: dict) -> str:
    """
    ç”Ÿæˆé…ç½®å”¯ä¸€æ ‡è¯† key

    ç›¸åŒé…ç½® = ç›¸åŒç¡¬ä»¶ + ç›¸åŒæ¨¡å‹ + ç›¸åŒ workload åœºæ™¯ + ç›¸åŒç²¾åº¦ + ç›¸åŒç‰ˆæœ¬
    """
    hw = entry.get("hardware", {})
    model = entry.get("model", {})
    cluster = entry.get("cluster")
    metadata = entry.get("metadata", {})

    # æå– workload åœºæ™¯å (e.g. 'Benchmark run: Q1' -> 'Q1')
    notes = metadata.get("notes", "")
    workload_name = "default"
    if notes:
        m = re.search(r"\b(Q\d+|M\d+|year\d+|stress|short|long|all)\b", notes, re.IGNORECASE)
        if m:
            workload_name = m.group(1).upper()

    # æ„å»ºé…ç½® key
    parts = [
        hw.get("chip_model", "unknown"),
        str(hw.get("chip_count", 1)),
        model.get("name", "unknown"),
        model.get("precision", "FP16"),
        workload_name,
        str(
            entry.get("sagellm_version") or entry.get("versions", {}).get("benchmark") or "unknown"
        ),
    ]

    # å¦‚æœæ˜¯å¤šèŠ‚ç‚¹ï¼ŒåŠ å…¥èŠ‚ç‚¹ä¿¡æ¯
    if cluster and cluster.get("node_count", 1) > 1:
        parts.append(f"nodes_{cluster['node_count']}")

    return "|".join(parts)


def is_better_result(new_entry: dict, existing_entry: dict) -> bool:
    """
    åˆ¤æ–­æ–°ç»“æœæ˜¯å¦æ¯”ç°æœ‰ç»“æœæ›´å¥½

    è¯„åˆ¤æ ‡å‡†ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
    1. throughput_tps è¶Šé«˜è¶Šå¥½
    2. ttft_ms è¶Šä½è¶Šå¥½
    3. error_rate è¶Šä½è¶Šå¥½
    """
    new_metrics = new_entry.get("metrics", {})
    old_metrics = existing_entry.get("metrics", {})

    # throughput é«˜æ›´å¥½
    new_tps = new_metrics.get("throughput_tps", 0)
    old_tps = old_metrics.get("throughput_tps", 0)
    if new_tps > old_tps * 1.05:  # 5% å®¹å·®
        return True
    if old_tps > new_tps * 1.05:
        return False

    # ttft ä½æ›´å¥½
    new_ttft = new_metrics.get("ttft_ms", float("inf"))
    old_ttft = old_metrics.get("ttft_ms", float("inf"))
    if new_ttft < old_ttft * 0.95:  # 5% å®¹å·®
        return True
    if old_ttft < new_ttft * 0.95:
        return False

    # error_rate ä½æ›´å¥½
    new_err = new_metrics.get("error_rate", 1)
    old_err = old_metrics.get("error_rate", 1)
    if new_err < old_err:
        return True

    # é»˜è®¤ä¿ç•™ç°æœ‰çš„ï¼ˆä¸è¦†ç›–ï¼‰
    return False


def sanitize_entry(entry: dict) -> dict:
    """ç¡®ä¿æ‰€æœ‰å­—æ®µç±»å‹ä¸€è‡´ï¼Œé¿å… HF Arrow schema å†²çªï¼ˆnull vs double/stringï¼‰"""
    hw = entry.get("hardware", {})
    env = entry.get("environment", {})

    # float å­—æ®µï¼šnull -> 0.0
    for key in ("memory_per_chip_gb", "total_memory_gb"):
        if hw.get(key) is None:
            hw[key] = 0.0

    # str å­—æ®µï¼šnull -> ""
    for key in ("cuda_version", "driver_version", "cann_version", "pytorch_version"):
        if env.get(key) is None:
            env[key] = ""

    return entry


def smart_merge(hf_latest: list[dict], user_data: list[dict]) -> list[dict]:
    """
    ä¸‰æ–¹æ™ºèƒ½åˆå¹¶

    å…³é”®è§„åˆ™ï¼š
    1. HF æœ€æ–°æ•°æ®ä¸ºåŸºå‡†ï¼ˆæƒå¨ç‰ˆæœ¬ï¼‰
    2. ç”¨æˆ·æ•°æ®è¿½åŠ æˆ–æ›´æ–°
    3. ç›¸åŒé…ç½®æ—¶ï¼Œé€‰æ‹©æ€§èƒ½æ›´å¥½çš„
    4. ä¸åŒé…ç½®åˆ™è¿½åŠ 

    è¿™æ ·å³ä½¿ç”¨æˆ·åŸºäºæ—§ç‰ˆæœ¬ HF æ•°æ®åˆå¹¶ï¼Œä¹Ÿèƒ½ä¸æœ€æ–°ç‰ˆæœ¬æ­£ç¡®åˆå¹¶ã€‚
    """
    merged: dict[str, dict] = {}

    # å…ˆåŠ å…¥ HF æœ€æ–°æ•°æ®ï¼ˆæƒå¨ç‰ˆæœ¬ï¼‰
    for entry in hf_latest:
        config_key = get_config_key(entry)
        merged[config_key] = sanitize_entry(entry)

    added = 0
    updated = 0
    skipped = 0

    # åˆå¹¶ç”¨æˆ·æ•°æ®
    for entry in user_data:
        entry = sanitize_entry(entry)
        config_key = get_config_key(entry)

        if config_key not in merged:
            # æ–°é…ç½®ï¼Œç›´æ¥æ·»åŠ 
            merged[config_key] = entry
            added += 1
            print(f"    âœ“ æ–°å¢: {config_key[:60]}...")
        else:
            # å·²å­˜åœ¨ï¼Œæ¯”è¾ƒæ€§èƒ½
            if is_better_result(entry, merged[config_key]):
                merged[config_key] = entry
                updated += 1
                print(f"    â†‘ æ›´æ–°: {config_key[:60]}...")
            else:
                skipped += 1
                # ä¸æ‰“å°è·³è¿‡çš„ï¼ˆå¤ªå¤šï¼‰

    print(f"\n  ğŸ“Š åˆå¹¶ç»“æœ: æ–°å¢ {added}, æ›´æ–° {updated}, è·³è¿‡ {skipped}, æ€»è®¡ {len(merged)}")
    return list(merged.values())


def main():
    print("=" * 60)
    print("ğŸ”€ å¹¶å‘å®‰å…¨åˆå¹¶ï¼ˆGitHub Actionsï¼‰")
    print("=" * 60)

    # è·¯å¾„è®¾ç½®
    hf_data_dir = Path("hf_data")

    if not hf_data_dir.exists():
        print("\nâŒ hf_data/ ç›®å½•ä¸å­˜åœ¨")
        print("ğŸ’¡ ç”¨æˆ·åº”è¯¥å…ˆè¿è¡Œ 'sagellm-benchmark aggregate'")
        exit(1)

    # 1. è¯»å–ç”¨æˆ·æäº¤çš„æ•°æ®
    print("\nğŸ“‚ è¯»å–ç”¨æˆ·æäº¤çš„æ•°æ®...")
    user_single_file = hf_data_dir / "leaderboard_single.json"
    user_multi_file = hf_data_dir / "leaderboard_multi.json"

    if not user_single_file.exists() and not user_multi_file.exists():
        print("  âš ï¸ ç¼ºå°‘æ‰€æœ‰å¿…è¦æ–‡ä»¶ï¼ˆleaderboard_single.json å’Œ leaderboard_multi.json å‡ä¸å­˜åœ¨ï¼‰")
        exit(1)

    user_single = (
        json.loads(user_single_file.read_text(encoding="utf-8"))
        if user_single_file.exists()
        else []
    )
    user_multi = (
        json.loads(user_multi_file.read_text(encoding="utf-8")) if user_multi_file.exists() else []
    )
    print(f"  âœ“ Single: {len(user_single)} æ¡")
    print(f"  âœ“ Multi: {len(user_multi)} æ¡")

    # 2. ä» HF ä¸‹è½½æœ€æ–°æ•°æ®ï¼ˆå¯èƒ½å·²è¢«å…¶ä»–ç”¨æˆ·æ›´æ–°ï¼‰
    print("\nğŸ“¥ ä» Hugging Face ä¸‹è½½æœ€æ–°æ•°æ®...")
    hf_single = download_from_hf("leaderboard_single.json")
    hf_multi = download_from_hf("leaderboard_multi.json")

    # 3. æ™ºèƒ½åˆå¹¶
    print("\nğŸ”€ æ™ºèƒ½åˆå¹¶ï¼ˆè§£å†³å¹¶å‘å†²çªï¼‰...")
    print("\n  Single (å•æœº):")
    merged_single = smart_merge(hf_single, user_single)

    print("\n  Multi (å¤šæœº):")
    merged_multi = smart_merge(hf_multi, user_multi)

    # 4. ä¿å­˜åˆå¹¶ç»“æœï¼ˆè¦†ç›–ç”¨æˆ·æäº¤çš„ç‰ˆæœ¬ï¼‰
    print("\nğŸ’¾ ä¿å­˜åˆå¹¶ç»“æœ...")
    user_single_file.write_text(
        json.dumps(merged_single, indent=2, ensure_ascii=False), encoding="utf-8"
    )
    user_multi_file.write_text(
        json.dumps(merged_multi, indent=2, ensure_ascii=False), encoding="utf-8"
    )
    print(f"  âœ“ {user_single_file}")
    print(f"  âœ“ {user_multi_file}")

    print("\nâœ… å¹¶å‘å®‰å…¨åˆå¹¶å®Œæˆï¼")
    print("ğŸ’¡ ä¸‹ä¸€æ­¥: è¿è¡Œ upload_to_hf.py ä¸Šä¼ åˆ° Hugging Face")


if __name__ == "__main__":
    main()
