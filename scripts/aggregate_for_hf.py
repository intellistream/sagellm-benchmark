#!/usr/bin/env python3
"""
èšåˆ outputs/ ç›®å½•ä¸‹çš„æ‰€æœ‰ benchmark ç»“æœï¼Œå¹¶ä¸ HF ç°æœ‰æ•°æ®åˆå¹¶

å…³é”®é€»è¾‘ï¼š
1. ä» HF ä¸‹è½½ç°æœ‰çš„ leaderboard æ•°æ®
2. åŠ è½½æœ¬åœ° outputs/ ä¸‹çš„æ–°ç»“æœ
3. åŸºäºé…ç½® key å»é‡åˆå¹¶ï¼ˆé€‰æ‹©æ€§èƒ½è¾ƒå¥½çš„ç»“æœï¼‰
4. ä¿å­˜åˆ° hf_data/ ç›®å½•

è¿è¡Œæ–¹å¼ï¼š
    python scripts/aggregate_for_hf.py
    
HF ä»“åº“ï¼š
    https://huggingface.co/datasets/wangyao36/sagellm-benchmark-results
"""
from __future__ import annotations

import json
import urllib.request
from pathlib import Path

# HF é…ç½®
HF_REPO = "wangyao36/sagellm-benchmark-results"
HF_BRANCH = "main"


def download_from_hf(filename: str) -> list[dict]:
    """ä» Hugging Face ä¸‹è½½ç°æœ‰æ•°æ®"""
    url = f"https://huggingface.co/datasets/{HF_REPO}/resolve/{HF_BRANCH}/{filename}"
    print(f"ğŸ“¥ ä¸‹è½½ HF æ•°æ®: {url}")

    try:
        with urllib.request.urlopen(url, timeout=30) as response:
            data = json.loads(response.read().decode("utf-8"))
            print(f"  âœ“ ä¸‹è½½æˆåŠŸ: {len(data)} æ¡è®°å½•")
            return data
    except urllib.error.HTTPError as e:
        if e.code == 404:
            print(f"  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆé¦–æ¬¡ä¸Šä¼ ï¼‰")
        else:
            print(f"  âš ï¸ HTTP é”™è¯¯ {e.code}: {e.reason}")
        return []
    except Exception as e:
        print(f"  âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
        return []


def load_local_results(outputs_dir: Path) -> list[dict]:
    """é€’å½’åŠ è½½ outputs ç›®å½•ä¸‹çš„æ‰€æœ‰ leaderboard JSON æ–‡ä»¶"""
    all_results = []

    for json_file in outputs_dir.rglob("*_leaderboard.json"):
        try:
            with open(json_file, encoding="utf-8") as f:
                data = json.load(f)
                all_results.append(data)
                print(f"  âœ“ åŠ è½½: {json_file.relative_to(outputs_dir)}")
        except Exception as e:
            print(f"  âœ— åŠ è½½å¤±è´¥: {json_file} - {e}")

    return all_results


def get_config_key(entry: dict) -> str:
    """
    ç”Ÿæˆé…ç½®å”¯ä¸€æ ‡è¯† key
    
    ç›¸åŒé…ç½® = ç›¸åŒç¡¬ä»¶ + ç›¸åŒæ¨¡å‹ + ç›¸åŒ workload + ç›¸åŒç²¾åº¦
    """
    hw = entry.get("hardware", {})
    model = entry.get("model", {})
    workload = entry.get("workload", {})
    cluster = entry.get("cluster")
    
    # æ„å»ºé…ç½® key
    parts = [
        hw.get("chip_model", "unknown"),
        str(hw.get("chip_count", 1)),
        model.get("name", "unknown"),
        model.get("precision", "FP16"),
        str(workload.get("input_length", 0)),
        str(workload.get("output_length", 0)),
    ]
    
    # å¦‚æœæ˜¯å¤šèŠ‚ç‚¹ï¼ŒåŠ å…¥èŠ‚ç‚¹ä¿¡æ¯
    if cluster and cluster.get("node_count", 1) > 1:
        parts.append(f"nodes_{cluster['node_count']}")
    
    return "|".join(parts)


def is_better_result(new_entry: dict, existing_entry: dict) -> bool:
    """
    åˆ¤æ–­æ–°ç»“æœæ˜¯å¦æ¯”ç°æœ‰ç»“æœæ›´å¥½
    
    è¯„åˆ¤æ ‡å‡†ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
    1. throughput_tps è¶Šé«˜è¶Šå¥½
    2. ttft_ms è¶Šä½è¶Šå¥½
    3. error_rate è¶Šä½è¶Šå¥½
    """
    new_metrics = new_entry.get("metrics", {})
    old_metrics = existing_entry.get("metrics", {})
    
    # throughput é«˜æ›´å¥½
    new_tps = new_metrics.get("throughput_tps", 0)
    old_tps = old_metrics.get("throughput_tps", 0)
    if new_tps > old_tps * 1.05:  # 5% å®¹å·®
        return True
    if old_tps > new_tps * 1.05:
        return False
    
    # ttft ä½æ›´å¥½
    new_ttft = new_metrics.get("ttft_ms", float("inf"))
    old_ttft = old_metrics.get("ttft_ms", float("inf"))
    if new_ttft < old_ttft * 0.95:  # 5% å®¹å·®
        return True
    if old_ttft < new_ttft * 0.95:
        return False
    
    # error_rate ä½æ›´å¥½
    new_err = new_metrics.get("error_rate", 1)
    old_err = old_metrics.get("error_rate", 1)
    if new_err < old_err:
        return True
    
    # é»˜è®¤ä¿ç•™ç°æœ‰çš„ï¼ˆä¸è¦†ç›–ï¼‰
    return False


def merge_results(existing: list[dict], new_results: list[dict]) -> list[dict]:
    """
    åˆå¹¶ç°æœ‰æ•°æ®å’Œæ–°æ•°æ®
    
    è§„åˆ™ï¼š
    - åŸºäºé…ç½® key å»é‡ï¼ˆç›¸åŒç¡¬ä»¶+æ¨¡å‹+workload+ç²¾åº¦ï¼‰
    - ç›¸åŒé…ç½®æ—¶ï¼Œä¿ç•™æ€§èƒ½æ›´å¥½çš„ç»“æœ
    - ä¸åŒé…ç½®åˆ™æ·»åŠ 
    """
    # ä½¿ç”¨ dict ä»¥ config_key ä¸º key è¿›è¡Œåˆå¹¶
    merged: dict[str, dict] = {}
    
    # å…ˆåŠ å…¥ç°æœ‰æ•°æ®
    for entry in existing:
        config_key = get_config_key(entry)
        merged[config_key] = entry
    
    added = 0
    updated = 0
    skipped = 0
    
    for entry in new_results:
        config_key = get_config_key(entry)
        
        if config_key not in merged:
            # æ–°é…ç½®ï¼Œç›´æ¥æ·»åŠ 
            merged[config_key] = entry
            added += 1
        else:
            # å·²å­˜åœ¨ï¼Œæ¯”è¾ƒæ€§èƒ½
            if is_better_result(entry, merged[config_key]):
                merged[config_key] = entry
                updated += 1
                print(f"    â†‘ æ›´æ–° (æ›´å¥½): {config_key[:50]}...")
            else:
                skipped += 1
                print(f"    â—‹ è·³è¿‡ (å·²æœ‰æ›´å¥½): {config_key[:50]}...")
    
    print(f"  ğŸ“Š åˆå¹¶ç»“æœ: æ–°å¢ {added}, æ›´æ–° {updated}, è·³è¿‡ {skipped}, æ€»è®¡ {len(merged)}")
    return list(merged.values())


def categorize_results(results: list[dict]) -> tuple[list, list, list]:
    """å°†ç»“æœåˆ†ç±»ä¸ºå•æœºå•å¡ã€å•æœºå¤šå¡ã€å¤šæœºå¤šå¡"""
    single_chip = []
    multi_chip = []
    multi_node = []

    for entry in results:
        chip_count = entry["hardware"]["chip_count"]
        cluster = entry.get("cluster")

        if cluster and cluster.get("node_count", 1) > 1:
            multi_node.append(entry)
        elif chip_count > 1:
            multi_chip.append(entry)
        else:
            single_chip.append(entry)

    return single_chip, multi_chip, multi_node


def main():
    # è·¯å¾„è®¾ç½®
    base_dir = Path(__file__).parent.parent
    outputs_dir = base_dir / "outputs"
    hf_output_dir = base_dir / "hf_data"

    # åˆ›å»ºè¾“å‡ºç›®å½•
    hf_output_dir.mkdir(exist_ok=True)

    # Step 1: ä» HF ä¸‹è½½ç°æœ‰æ•°æ®
    print(f"\nğŸ“¡ ä» Hugging Face ä¸‹è½½ç°æœ‰æ•°æ®...")
    existing_single = download_from_hf("leaderboard_single.json")
    existing_multi = download_from_hf("leaderboard_multi.json")

    # Step 2: åŠ è½½æœ¬åœ°æ–°ç»“æœ
    print(f"\nğŸ“‚ ä»æœ¬åœ° {outputs_dir} åŠ è½½æ–°ç»“æœ...")
    if not outputs_dir.exists():
        print(f"  âš ï¸ outputs ç›®å½•ä¸å­˜åœ¨ï¼Œä»…ä½¿ç”¨ HF ç°æœ‰æ•°æ®")
        local_results = []
    else:
        local_results = load_local_results(outputs_dir)
        print(f"  ğŸ“Š åŠ è½½äº† {len(local_results)} æ¡æœ¬åœ°ç»“æœ")

    # Step 3: åˆ†ç±»æœ¬åœ°ç»“æœ
    if local_results:
        local_single_chip, local_multi_chip, local_multi_node = categorize_results(
            local_results
        )
        local_single = local_single_chip + local_multi_chip
    else:
        local_single = []
        local_multi_node = []

    # Step 4: åˆå¹¶æ•°æ®
    print(f"\nğŸ”€ åˆå¹¶æ•°æ®...")
    print(f"  Single (å•æœºå•å¡+å¤šå¡):")
    merged_single = merge_results(existing_single, local_single)
    print(f"  Multi (å¤šæœºå¤šå¡):")
    merged_multi = merge_results(existing_multi, local_multi_node)

    # Step 5: ä¿å­˜åˆ° JSON æ–‡ä»¶
    single_file = hf_output_dir / "leaderboard_single.json"
    multi_file = hf_output_dir / "leaderboard_multi.json"

    with open(single_file, "w", encoding="utf-8") as f:
        json.dump(merged_single, f, indent=2, ensure_ascii=False)

    with open(multi_file, "w", encoding="utf-8") as f:
        json.dump(merged_multi, f, indent=2, ensure_ascii=False)

    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nâœ… èšåˆå®Œæˆï¼")
    print(f"  ğŸ“„ {single_file.name}: {len(merged_single)} æ¡")
    print(f"  ğŸ“„ {multi_file.name}: {len(merged_multi)} æ¡")
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥: è¿è¡Œ scripts/upload_to_hf.py ä¸Šä¼ åˆ° HF")


if __name__ == "__main__":
    main()
