#!/usr/bin/env python3
"""
ç”¨æˆ·æœ¬åœ°èšåˆå‘½ä»¤ï¼šä» HF æ‹‰å–æœ€æ–°æ•°æ®å¹¶ä¸æœ¬åœ°ç»“æœåˆå¹¶

è¿™æ˜¯ç”¨æˆ·åœ¨æœ¬åœ°è¿è¡Œçš„å‘½ä»¤ï¼Œç”¨äºå‡†å¤‡ä¸Šä¼ åˆ° GitHub çš„æ•°æ®ã€‚

å·¥ä½œæµç¨‹ï¼š
1. ä» HF ä¸‹è½½å…¬å¼€çš„ leaderboard æ•°æ®ï¼ˆæ— éœ€ tokenï¼‰
2. æ‰«ææœ¬åœ° outputs/ ç›®å½•çš„æ–°ç»“æœ
3. æ™ºèƒ½åˆå¹¶ï¼ˆå»é‡ï¼Œé€‰æ€§èƒ½æ›´å¥½çš„ï¼‰
4. ä¿å­˜åˆ° hf_data/ ç›®å½•
5. ç”¨æˆ·æäº¤ hf_data/ åˆ° gitï¼ˆä¸æäº¤ outputs/ï¼‰

è¿è¡Œæ–¹å¼ï¼š
    python scripts/aggregate_for_hf.py
    æˆ–
    sagellm-benchmark aggregate

HF ä»“åº“ï¼ˆå…¬å¼€è®¿é—®ï¼‰ï¼š
    https://huggingface.co/datasets/intellistream/sagellm-benchmark-results
"""

from __future__ import annotations

import json
import os
import re
import urllib.request
from pathlib import Path

# HF é…ç½®
HF_REPO = "intellistream/sagellm-benchmark-results"
HF_BRANCH = "main"


def download_from_hf(filename: str) -> list[dict]:
    """
    ä» Hugging Face ä¸‹è½½ç°æœ‰æ•°æ®

    ç«¯ç‚¹é€‰æ‹©ç­–ç•¥ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
    1. ç¯å¢ƒå˜é‡ HF_ENDPOINTï¼ˆå¦‚æœè®¾ç½®ï¼‰
    2. å®˜æ–¹åœ°å€ https://huggingface.coï¼ˆé»˜è®¤ï¼‰
    3. å¦‚æœå®˜æ–¹å¤±è´¥ï¼Œè‡ªåŠ¨å›é€€åˆ° https://hf-mirror.com
    """
    # 1. ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„ç«¯ç‚¹
    endpoint = os.getenv("HF_ENDPOINT", "https://huggingface.co")

    # 2. å®šä¹‰å¤‡ç”¨ç«¯ç‚¹åˆ—è¡¨ï¼ˆå¦‚æœä¸»ç«¯ç‚¹å¤±è´¥ï¼‰
    fallback_endpoints = []
    if endpoint != "https://hf-mirror.com":
        # å¦‚æœå½“å‰ä¸æ˜¯é•œåƒï¼Œå°†é•œåƒä½œä¸ºå¤‡ç”¨
        fallback_endpoints.append("https://hf-mirror.com")
    if endpoint != "https://huggingface.co":
        # å¦‚æœå½“å‰ä¸æ˜¯å®˜æ–¹ï¼Œå°†å®˜æ–¹ä½œä¸ºå¤‡ç”¨
        fallback_endpoints.append("https://huggingface.co")

    # 3. å°è¯•ä¸»ç«¯ç‚¹
    endpoints_to_try = [endpoint] + fallback_endpoints

    for idx, ep in enumerate(endpoints_to_try):
        url = f"{ep}/datasets/{HF_REPO}/resolve/{HF_BRANCH}/{filename}"
        is_primary = idx == 0
        prefix = "ğŸ“¥ ä¸‹è½½ HF æ•°æ®:" if is_primary else "  ğŸ”„ å›é€€åˆ°:"

        print(f"{prefix} {url}")

        try:
            with urllib.request.urlopen(url, timeout=30) as response:
                data = json.loads(response.read().decode("utf-8"))
                print(f"  âœ“ ä¸‹è½½æˆåŠŸ: {len(data)} æ¡è®°å½•")
                return data
        except urllib.error.HTTPError as e:
            if e.code == 404:
                print("  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆé¦–æ¬¡ä¸Šä¼ ï¼‰")
                return []  # 404 æ˜¯ç¡®å®šçš„ï¼Œæ— éœ€é‡è¯•
            else:
                print(f"  âš ï¸ HTTP é”™è¯¯ {e.code}: {e.reason}")
                if idx < len(endpoints_to_try) - 1:
                    continue  # å°è¯•ä¸‹ä¸€ä¸ªç«¯ç‚¹
                return []
        except Exception as e:
            print(f"  âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
            if idx < len(endpoints_to_try) - 1:
                continue  # å°è¯•ä¸‹ä¸€ä¸ªç«¯ç‚¹
            return []

    return []


def load_local_results(outputs_dir: Path) -> list[dict]:
    """é€’å½’åŠ è½½ outputs ç›®å½•ä¸‹çš„æ‰€æœ‰ leaderboard JSON æ–‡ä»¶"""
    all_results = []

    for json_file in outputs_dir.rglob("*_leaderboard.json"):
        try:
            with open(json_file, encoding="utf-8") as f:
                data = json.load(f)
                all_results.append(data)
                print(f"  âœ“ åŠ è½½: {json_file.relative_to(outputs_dir)}")
        except Exception as e:
            print(f"  âœ— åŠ è½½å¤±è´¥: {json_file} - {e}")

    return all_results


def get_config_key(entry: dict) -> str:
    """
    ç”Ÿæˆé…ç½®å”¯ä¸€æ ‡è¯† key

    ç›¸åŒé…ç½® = ç›¸åŒç¡¬ä»¶ + ç›¸åŒæ¨¡å‹ + ç›¸åŒ workload åœºæ™¯ + ç›¸åŒç²¾åº¦ + ç›¸åŒç‰ˆæœ¬
    """
    hw = entry.get("hardware", {})
    model = entry.get("model", {})
    cluster = entry.get("cluster")
    metadata = entry.get("metadata", {})

    # æå– workload åœºæ™¯å (e.g. 'Benchmark run: Q1' -> 'Q1')
    notes = metadata.get("notes", "")
    workload_name = "default"
    if notes:
        m = re.search(r"\b(Q\d+|M\d+|year\d+|stress|short|long|all)\b", notes, re.IGNORECASE)
        if m:
            workload_name = m.group(1).upper()

    # æ„å»ºé…ç½® key
    parts = [
        hw.get("chip_model", "unknown"),
        str(hw.get("chip_count", 1)),
        model.get("name", "unknown"),
        model.get("precision", "FP16"),
        workload_name,
        str(
            entry.get("sagellm_version") or entry.get("versions", {}).get("benchmark") or "unknown"
        ),
    ]

    # å¦‚æœæ˜¯å¤šèŠ‚ç‚¹ï¼ŒåŠ å…¥èŠ‚ç‚¹ä¿¡æ¯
    if cluster and cluster.get("node_count", 1) > 1:
        parts.append(f"nodes_{cluster['node_count']}")

    return "|".join(parts)


def is_better_result(new_entry: dict, existing_entry: dict) -> bool:
    """
    åˆ¤æ–­æ–°ç»“æœæ˜¯å¦æ¯”ç°æœ‰ç»“æœæ›´å¥½

    è¯„åˆ¤æ ‡å‡†ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
    1. throughput_tps è¶Šé«˜è¶Šå¥½
    2. ttft_ms è¶Šä½è¶Šå¥½
    3. error_rate è¶Šä½è¶Šå¥½
    """
    new_metrics = new_entry.get("metrics", {})
    old_metrics = existing_entry.get("metrics", {})

    # throughput é«˜æ›´å¥½
    new_tps = new_metrics.get("throughput_tps", 0)
    old_tps = old_metrics.get("throughput_tps", 0)
    if new_tps > old_tps * 1.05:  # 5% å®¹å·®
        return True
    if old_tps > new_tps * 1.05:
        return False

    # ttft ä½æ›´å¥½
    new_ttft = new_metrics.get("ttft_ms", float("inf"))
    old_ttft = old_metrics.get("ttft_ms", float("inf"))
    if new_ttft < old_ttft * 0.95:  # 5% å®¹å·®
        return True
    if old_ttft < new_ttft * 0.95:
        return False

    # error_rate ä½æ›´å¥½
    new_err = new_metrics.get("error_rate", 1)
    old_err = old_metrics.get("error_rate", 1)
    if new_err < old_err:
        return True

    # é»˜è®¤ä¿ç•™ç°æœ‰çš„ï¼ˆä¸è¦†ç›–ï¼‰
    return False


def sanitize_entry(entry: dict) -> dict:
    """ç¡®ä¿æ‰€æœ‰å­—æ®µç±»å‹ä¸€è‡´ï¼Œé¿å… HF Arrow schema å†²çªï¼ˆnull vs double/stringï¼‰"""
    hw = entry.get("hardware", {})
    env = entry.get("environment", {})

    # float å­—æ®µï¼šnull -> 0.0
    for key in ("memory_per_chip_gb", "total_memory_gb"):
        if hw.get(key) is None:
            hw[key] = 0.0

    # str å­—æ®µï¼šnull -> ""
    for key in ("cuda_version", "driver_version", "cann_version", "pytorch_version"):
        if env.get(key) is None:
            env[key] = ""

    return entry


def merge_results(existing: list[dict], new_results: list[dict]) -> list[dict]:
    """
    åˆå¹¶ç°æœ‰æ•°æ®å’Œæ–°æ•°æ®

    è§„åˆ™ï¼š
    - åŸºäºé…ç½® key å»é‡ï¼ˆç›¸åŒç¡¬ä»¶+æ¨¡å‹+workload+ç²¾åº¦ï¼‰
    - ç›¸åŒé…ç½®æ—¶ï¼Œä¿ç•™æ€§èƒ½æ›´å¥½çš„ç»“æœ
    - ä¸åŒé…ç½®åˆ™æ·»åŠ 
    """
    # ä½¿ç”¨ dict ä»¥ config_key ä¸º key è¿›è¡Œåˆå¹¶
    merged: dict[str, dict] = {}

    # å…ˆåŠ å…¥ç°æœ‰æ•°æ®
    for entry in existing:
        config_key = get_config_key(entry)
        merged[config_key] = sanitize_entry(entry)

    added = 0
    updated = 0
    skipped = 0

    for entry in new_results:
        config_key = get_config_key(entry)
        entry = sanitize_entry(entry)

        if config_key not in merged:
            # æ–°é…ç½®ï¼Œç›´æ¥æ·»åŠ 
            merged[config_key] = entry
            added += 1
        else:
            # å·²å­˜åœ¨ï¼Œæ¯”è¾ƒæ€§èƒ½
            if is_better_result(entry, merged[config_key]):
                merged[config_key] = entry
                updated += 1
                print(f"    â†‘ æ›´æ–° (æ›´å¥½): {config_key[:50]}...")
            else:
                skipped += 1
                print(f"    â—‹ è·³è¿‡ (å·²æœ‰æ›´å¥½): {config_key[:50]}...")

    print(f"  ğŸ“Š åˆå¹¶ç»“æœ: æ–°å¢ {added}, æ›´æ–° {updated}, è·³è¿‡ {skipped}, æ€»è®¡ {len(merged)}")
    return list(merged.values())


def categorize_results(results: list[dict]) -> tuple[list, list, list]:
    """å°†ç»“æœåˆ†ç±»ä¸ºå•æœºå•å¡ã€å•æœºå¤šå¡ã€å¤šæœºå¤šå¡"""
    single_chip = []
    multi_chip = []
    multi_node = []

    for entry in results:
        chip_count = entry["hardware"]["chip_count"]
        cluster = entry.get("cluster")

        if cluster and cluster.get("node_count", 1) > 1:
            multi_node.append(entry)
        elif chip_count > 1:
            multi_chip.append(entry)
        else:
            single_chip.append(entry)

    return single_chip, multi_chip, multi_node


def main():
    print("=" * 70)
    print("ğŸ“¦ sageLLM Benchmark - æœ¬åœ°èšåˆå·¥å…·")
    print("=" * 70)

    # è·¯å¾„è®¾ç½®
    base_dir = Path(__file__).parent.parent
    outputs_dir = base_dir / "outputs"
    hf_output_dir = base_dir / "hf_data"

    # åˆ›å»ºè¾“å‡ºç›®å½•
    hf_output_dir.mkdir(exist_ok=True)

    # Step 1: ä» HF ä¸‹è½½ç°æœ‰æ•°æ®ï¼ˆå…¬å¼€è®¿é—®ï¼Œæ— éœ€ tokenï¼‰
    print("\nğŸ“¥ ä» Hugging Face ä¸‹è½½æœ€æ–°æ•°æ®...")
    print(f"   ä»“åº“: https://huggingface.co/datasets/{HF_REPO}")
    existing_single = download_from_hf("leaderboard_single.json")
    existing_multi = download_from_hf("leaderboard_multi.json")

    # Step 2: åŠ è½½æœ¬åœ°æ–°ç»“æœ
    print("\nğŸ“‚ æ‰«ææœ¬åœ° outputs/ ç›®å½•...")
    if not outputs_dir.exists():
        print("  âš ï¸ outputs/ ç›®å½•ä¸å­˜åœ¨")
        print("  ğŸ’¡ è¯·å…ˆè¿è¡Œ benchmark: sagellm-benchmark run --model <model>")
        local_results = []
    else:
        local_results = load_local_results(outputs_dir)
        if not local_results:
            print("  âš ï¸ æœªæ‰¾åˆ°ä»»ä½• *_leaderboard.json æ–‡ä»¶")
            print("  ğŸ’¡ è¯·å…ˆè¿è¡Œ benchmark ç”Ÿæˆç»“æœ")
        else:
            print(f"  âœ“ æ‰¾åˆ° {len(local_results)} æ¡æœ¬åœ°ç»“æœ")

    # Step 3: åˆ†ç±»æœ¬åœ°ç»“æœ
    if local_results:
        local_single_chip, local_multi_chip, local_multi_node = categorize_results(local_results)
        local_single = local_single_chip + local_multi_chip
        print(f"  â””â”€ å•æœº: {len(local_single)} æ¡, å¤šæœº: {len(local_multi_node)} æ¡")
    else:
        local_single = []
        local_multi_node = []

    # Step 4: åˆå¹¶æ•°æ®
    print("\nğŸ”€ æ™ºèƒ½åˆå¹¶æ•°æ®...")
    print("  Single (å•æœºå•å¡+å¤šå¡):")
    merged_single = merge_results(existing_single, local_single)
    print("  Multi (å¤šæœºå¤šå¡):")
    merged_multi = merge_results(existing_multi, local_multi_node)

    # Step 5: ä¿å­˜åˆ° JSON æ–‡ä»¶
    print("\nğŸ’¾ ä¿å­˜åˆ° hf_data/ ç›®å½•...")
    single_file = hf_output_dir / "leaderboard_single.json"
    multi_file = hf_output_dir / "leaderboard_multi.json"

    with open(single_file, "w", encoding="utf-8") as f:
        json.dump(merged_single, f, indent=2, ensure_ascii=False)

    with open(multi_file, "w", encoding="utf-8") as f:
        json.dump(merged_multi, f, indent=2, ensure_ascii=False)

    print(f"  âœ“ {single_file} ({len(merged_single)} æ¡)")
    print(f"  âœ“ {multi_file} ({len(merged_multi)} æ¡)")

    # å‹å¥½æç¤º
    print("\n" + "=" * 70)
    print("âœ… èšåˆå®Œæˆï¼")
    print("=" * 70)
    print("\nğŸ“Œ ä¸‹ä¸€æ­¥æ“ä½œï¼š")
    print("  1. æäº¤èšåˆæ•°æ®åˆ° git:")
    print("     git add hf_data/")
    print("     git commit -m 'feat: add benchmark results'")
    print("     git push")
    print("\n  2. GitHub Actions ä¼šè‡ªåŠ¨:")
    print("     - ä¸ HF æœ€æ–°æ•°æ®åˆå¹¶ï¼ˆè§£å†³å¹¶å‘å†²çªï¼‰")
    print("     - ä¸Šä¼ åˆ° Hugging Face")
    print("     - æ¸…ç† hf_data/ ä¿æŒä»“åº“è½»é‡")
    print("\nğŸ’¡ æç¤º: outputs/ ç›®å½•ä¸ä¼šè¢«æäº¤ï¼ˆåœ¨ .gitignore ä¸­ï¼‰")
    print("=" * 70)


if __name__ == "__main__":
    main()
