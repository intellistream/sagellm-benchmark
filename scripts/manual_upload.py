#!/usr/bin/env python3
"""
æ‰‹åŠ¨ä¸Šä¼  Benchmark ç»“æœåˆ° Hugging Face

åŠŸèƒ½ï¼š
1. ä» HF æ‹‰å–ç°æœ‰æ•°æ®
2. ä¸æœ¬åœ° outputs/ æ•°æ®åˆå¹¶ï¼ˆç›¸åŒé…ç½®ä¿ç•™æ›´å¥½çš„ç»“æœï¼‰
3. æ¨é€åˆ° HF

ä½¿ç”¨æ–¹æ³•ï¼š
    # è®¾ç½® HF_TOKEN ç¯å¢ƒå˜é‡
    export HF_TOKEN=hf_xxx
    
    # è¿è¡Œè„šæœ¬
    python scripts/manual_upload.py
    
    # æˆ–ä¸€è¡Œå‘½ä»¤
    HF_TOKEN=hf_xxx python scripts/manual_upload.py
"""
from __future__ import annotations

import json
import os
import sys
import urllib.request
from datetime import datetime
from pathlib import Path

# =============================================================================
# é…ç½®
# =============================================================================

HF_REPO = "wangyao36/sagellm-benchmark-results"
HF_BRANCH = "main"
# æ”¯æŒ HF é•œåƒç«™ï¼ˆä¸­å›½ç”¨æˆ·å¯è®¾ç½® HF_ENDPOINT=https://hf-mirror.comï¼‰
HF_ENDPOINT = os.environ.get("HF_ENDPOINT", "https://huggingface.co")

# è·¯å¾„
BASE_DIR = Path(__file__).parent.parent
OUTPUTS_DIR = BASE_DIR / "outputs"
HF_DATA_DIR = BASE_DIR / "hf_data"


# =============================================================================
# Step 1: ä» HF ä¸‹è½½ç°æœ‰æ•°æ®
# =============================================================================

def download_from_hf(filename: str) -> list[dict]:
    """ä» Hugging Face ä¸‹è½½ç°æœ‰æ•°æ®"""
    url = f"{HF_ENDPOINT}/datasets/{HF_REPO}/resolve/{HF_BRANCH}/{filename}"
    print(f"  ğŸ“¥ ä¸‹è½½: {url}")

    try:
        with urllib.request.urlopen(url, timeout=30) as response:
            data = json.loads(response.read().decode("utf-8"))
            print(f"     âœ“ æˆåŠŸ: {len(data)} æ¡è®°å½•")
            return data
    except urllib.error.HTTPError as e:
        if e.code == 404:
            print(f"     âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆé¦–æ¬¡ä¸Šä¼ ï¼‰")
        else:
            print(f"     âš ï¸ HTTP é”™è¯¯ {e.code}: {e.reason}")
        return []
    except Exception as e:
        print(f"     âš ï¸ ä¸‹è½½å¤±è´¥: {e}")
        return []


# =============================================================================
# Step 2: åŠ è½½æœ¬åœ°æ•°æ®
# =============================================================================

def load_local_results() -> list[dict]:
    """é€’å½’åŠ è½½ outputs ç›®å½•ä¸‹çš„æ‰€æœ‰ leaderboard JSON æ–‡ä»¶"""
    all_results = []

    if not OUTPUTS_DIR.exists():
        print(f"  âš ï¸ outputs ç›®å½•ä¸å­˜åœ¨")
        return []

    for json_file in OUTPUTS_DIR.rglob("*_leaderboard.json"):
        try:
            with open(json_file, encoding="utf-8") as f:
                data = json.load(f)
                all_results.append(data)
                print(f"  âœ“ åŠ è½½: {json_file.relative_to(OUTPUTS_DIR)}")
        except Exception as e:
            print(f"  âœ— åŠ è½½å¤±è´¥: {json_file} - {e}")

    return all_results


# =============================================================================
# Step 3: æ™ºèƒ½åˆå¹¶
# =============================================================================

def get_config_key(entry: dict) -> str:
    """ç”Ÿæˆé…ç½®å”¯ä¸€æ ‡è¯† key"""
    hw = entry.get("hardware", {})
    model = entry.get("model", {})
    workload = entry.get("workload", {})
    cluster = entry.get("cluster")

    parts = [
        hw.get("chip_model", "unknown"),
        str(hw.get("chip_count", 1)),
        model.get("name", "unknown"),
        model.get("precision", "FP16"),
        str(workload.get("input_length", 0)),
        str(workload.get("output_length", 0)),
    ]

    if cluster and cluster.get("node_count", 1) > 1:
        parts.append(f"nodes_{cluster['node_count']}")

    return "|".join(parts)


def is_better_result(new_entry: dict, existing_entry: dict) -> bool:
    """åˆ¤æ–­æ–°ç»“æœæ˜¯å¦æ¯”ç°æœ‰ç»“æœæ›´å¥½"""
    new_metrics = new_entry.get("metrics", {})
    old_metrics = existing_entry.get("metrics", {})

    # throughput é«˜æ›´å¥½
    new_tps = new_metrics.get("throughput_tps", 0)
    old_tps = old_metrics.get("throughput_tps", 0)
    if new_tps > old_tps * 1.05:
        return True
    if old_tps > new_tps * 1.05:
        return False

    # ttft ä½æ›´å¥½
    new_ttft = new_metrics.get("ttft_ms", float("inf"))
    old_ttft = old_metrics.get("ttft_ms", float("inf"))
    if new_ttft < old_ttft * 0.95:
        return True
    if old_ttft < new_ttft * 0.95:
        return False

    # error_rate ä½æ›´å¥½
    new_err = new_metrics.get("error_rate", 1)
    old_err = old_metrics.get("error_rate", 1)
    if new_err < old_err:
        return True

    return False


def merge_results(existing: list[dict], new_results: list[dict]) -> list[dict]:
    """åˆå¹¶ç°æœ‰æ•°æ®å’Œæ–°æ•°æ®"""
    merged: dict[str, dict] = {}

    # å…ˆåŠ å…¥ç°æœ‰æ•°æ®
    for entry in existing:
        config_key = get_config_key(entry)
        merged[config_key] = entry

    added = 0
    updated = 0
    skipped = 0

    for entry in new_results:
        config_key = get_config_key(entry)

        if config_key not in merged:
            merged[config_key] = entry
            added += 1
        else:
            if is_better_result(entry, merged[config_key]):
                merged[config_key] = entry
                updated += 1
            else:
                skipped += 1

    print(f"     ğŸ“Š æ–°å¢ {added}, æ›´æ–° {updated}, è·³è¿‡ {skipped}, æ€»è®¡ {len(merged)}")
    return list(merged.values())


def categorize_results(results: list[dict]) -> tuple[list, list]:
    """å°†ç»“æœåˆ†ç±»ä¸ºå•æœºå’Œå¤šæœº"""
    single = []
    multi = []

    for entry in results:
        cluster = entry.get("cluster")
        if cluster and cluster.get("node_count", 1) > 1:
            multi.append(entry)
        else:
            single.append(entry)

    return single, multi


# =============================================================================
# Step 4: ä¸Šä¼ åˆ° HF
# =============================================================================

def upload_to_hf(token: str) -> None:
    """ä¸Šä¼ æ–‡ä»¶åˆ° Hugging Face"""
    try:
        from huggingface_hub import HfApi, login
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£… huggingface_hub: pip install huggingface_hub")
        sys.exit(1)

    login(token=token)
    api = HfApi()

    # ç¡®ä¿ repo å­˜åœ¨
    try:
        api.repo_info(repo_id=HF_REPO, repo_type="dataset")
        print(f"  âœ“ Repo å­˜åœ¨: {HF_REPO}")
    except Exception:
        print(f"  ğŸ“¦ åˆ›å»º Repo: {HF_REPO}")
        api.create_repo(repo_id=HF_REPO, repo_type="dataset", private=False)

    # ä¸Šä¼ æ–‡ä»¶
    files = [
        HF_DATA_DIR / "leaderboard_single.json",
        HF_DATA_DIR / "leaderboard_multi.json",
    ]

    for local_path in files:
        if not local_path.exists():
            print(f"  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {local_path}")
            continue

        print(f"  ğŸ“¤ ä¸Šä¼ : {local_path.name}")
        api.upload_file(
            path_or_fileobj=str(local_path),
            path_in_repo=local_path.name,
            repo_id=HF_REPO,
            repo_type="dataset",
            commit_message=f"Update {local_path.name} - {datetime.now().isoformat()}",
        )
        print(f"     âœ“ å®Œæˆ")


# =============================================================================
# Main
# =============================================================================

def main():
    print("=" * 60)
    print("ğŸ“¦ sagellm-benchmark æ‰‹åŠ¨ä¸Šä¼ åˆ° Hugging Face")
    print("=" * 60)

    # æ£€æŸ¥ token
    token = os.environ.get("HF_TOKEN")
    if not token:
        print("\nâŒ é”™è¯¯: HF_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        print("\nè¯·è®¾ç½® HF_TOKEN:")
        print("  export HF_TOKEN=hf_xxx")
        print("\næˆ–è€…:")
        print("  HF_TOKEN=hf_xxx python scripts/manual_upload.py")
        sys.exit(1)

    print(f"\nâœ… HF_TOKEN å·²è®¾ç½®")
    print(f"ğŸ“ HF ä»“åº“: {HF_REPO}")

    # Step 1: ä» HF æ‹‰å–ç°æœ‰æ•°æ®
    print("\n" + "-" * 60)
    print("Step 1: ä» Hugging Face æ‹‰å–ç°æœ‰æ•°æ®")
    print("-" * 60)
    existing_single = download_from_hf("leaderboard_single.json")
    existing_multi = download_from_hf("leaderboard_multi.json")

    # Step 2: åŠ è½½æœ¬åœ°æ•°æ®
    print("\n" + "-" * 60)
    print("Step 2: åŠ è½½æœ¬åœ° outputs/ æ•°æ®")
    print("-" * 60)
    local_results = load_local_results()
    print(f"\n  ğŸ“Š å…±åŠ è½½ {len(local_results)} æ¡æœ¬åœ°ç»“æœ")

    if not local_results and not existing_single and not existing_multi:
        print("\nâš ï¸ æ²¡æœ‰ä»»ä½•æ•°æ®å¯ä¸Šä¼ ")
        sys.exit(0)

    # Step 3: åˆ†ç±»å¹¶åˆå¹¶
    print("\n" + "-" * 60)
    print("Step 3: æ™ºèƒ½åˆå¹¶æ•°æ®")
    print("-" * 60)

    local_single, local_multi = categorize_results(local_results)

    print(f"\n  Single (å•æœº):")
    merged_single = merge_results(existing_single, local_single)

    print(f"\n  Multi (å¤šæœº):")
    merged_multi = merge_results(existing_multi, local_multi)

    # ä¿å­˜åˆ°æœ¬åœ°
    HF_DATA_DIR.mkdir(exist_ok=True)

    single_file = HF_DATA_DIR / "leaderboard_single.json"
    multi_file = HF_DATA_DIR / "leaderboard_multi.json"

    with open(single_file, "w", encoding="utf-8") as f:
        json.dump(merged_single, f, indent=2, ensure_ascii=False)

    with open(multi_file, "w", encoding="utf-8") as f:
        json.dump(merged_multi, f, indent=2, ensure_ascii=False)

    print(f"\n  ğŸ’¾ å·²ä¿å­˜åˆ° hf_data/")
    print(f"     - {single_file.name}: {len(merged_single)} æ¡")
    print(f"     - {multi_file.name}: {len(merged_multi)} æ¡")

    # Step 4: ä¸Šä¼ åˆ° HF
    print("\n" + "-" * 60)
    print("Step 4: ä¸Šä¼ åˆ° Hugging Face")
    print("-" * 60)
    upload_to_hf(token)

    # å®Œæˆ
    print("\n" + "=" * 60)
    print("âœ… å®Œæˆï¼")
    print(f"ğŸ”— https://huggingface.co/datasets/{HF_REPO}")
    print("=" * 60)


if __name__ == "__main__":
    main()
